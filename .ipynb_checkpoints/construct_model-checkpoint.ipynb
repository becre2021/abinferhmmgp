{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "sys.path.append('./../PYTORCH_GP/')\n",
    "# from PYTORCH_GP.models.ssgpr_reparametrization import ssgpr_amoritized_reparametrization_sm as ssgpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import likelihoods\n",
    "from mean_function import zero_function\n",
    "\n",
    "from time import time\n",
    "TensorType = torch.DoubleTensor\n",
    "numpy_Type = np.float64\n",
    "\n",
    "\n",
    "class gpmodel(model):\n",
    "    def __init__(self, x, y, kernel, meanfunction, likelihood , cuda_option  , name = 'gp'):\n",
    "        super(gpmodel,self).__init__()\n",
    "        self.kernel = kernel\n",
    "        self.likelihood = likelihoods.Gaussian(variance = 0.1,cuda_option = cuda_option) if likelihood == None else likelihood\n",
    "        self.meanfunction = zero_function(cuda_option) if meanfunction == None else meanfunction(cuda_option)\n",
    "        self.tensor_type = torch.cuda.DoubleTensor if cuda_option == True else torch.DoubleTensor\n",
    "        self.zitter = 1e-8\n",
    "        self._check_observations(x,y)\n",
    "        self.__class__.__name__ = name\n",
    "\n",
    "    def _check_observations(self,x,y):\n",
    "                    \n",
    "        self.x = torch.from_numpy(x).type(self.tensor_type) if x != None else None\n",
    "        self.y = torch.from_numpy(x).type(self.tensor_type) if y != None else None\n",
    "        return\n",
    "\n",
    "    def compute_loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_Kxx(self,x):\n",
    "        num_input = x.shape[0]\n",
    "        return self.kernel.K(x) + (self.likelihood.variance.transform() + self.zitter).expand(num_input, num_input).diag().diag()\n",
    "        #return self.kernel.K(x)\n",
    "\n",
    "\n",
    "    def _compute_Kxx_diag(self,x):\n",
    "        #return self._compute_Kxx(x).diag()\n",
    "        return self._compute_Kxx(x).diag()\n",
    "    \n",
    "    def _compute_Kxs(self,x,xstar):\n",
    "        num_input = x.shape[0]\n",
    "        return self.kernel.K(x,xstar)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from model import Param\n",
    "from models.gpmodel import gpmodel\n",
    "from function import trtrs, cholesky, lt_log_determinant\n",
    "from kernels.RBF_kernel import RBF\n",
    "from mean_function import zero_function\n",
    "import likelihoods\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "TensorType = torch.DoubleTensor\n",
    "\n",
    "\n",
    "class gpr(gpmodel):\n",
    "\n",
    "    def __init__(self, inputs, outputs, kernel, mean_function = None, likelihood = None, cuda_option = None):\n",
    "        # input_obs = # training_data x # data_dim\n",
    "        # output_obs = # training_data x 1\n",
    "        super(gpr,self).__init__(inputs, outputs, kernel, mean_function, likelihood, cuda_option,name= 'gpr')\n",
    "\n",
    "        \n",
    "    def _check_observations(self,x,y):                    \n",
    "        self.x = torch.from_numpy(x).type(self.tensor_type) if x != None else None\n",
    "        self.y = torch.from_numpy(x).type(self.tensor_type) if y != None else None\n",
    "        return\n",
    "        \n",
    "    def compute_loss(self, batch_x, batch_y ):\n",
    "        num_input,dim_output = batch_y.shape\n",
    "        # made by me\n",
    "        L = cholesky(self._compute_Kxx(batch_x))\n",
    "        alpha = trtrs(L, batch_y)\n",
    "        loss = 0.5 * alpha.pow(2).sum() + lt_log_determinant(L) + 0.5 * num_input * np.log(2.0 * np.pi)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _get_log_prob(self, batch_x, batch_y ) : \n",
    "        return -self.compute_loss(batch_x, batch_y)\n",
    "\n",
    "    \n",
    "    def _predict(self, inputs_new, diag):\n",
    "        if isinstance(inputs_new, np.ndarray):\n",
    "            inputs_new = torch.Tensor(inputs_new).type(self.tensor_type)\n",
    "        kxx = self._compute_Kxx(self.x)\n",
    "        k_xs = self._compute_Kxs(self.x,inputs_new)\n",
    "        # reference_code\n",
    "        L = cholesky(kxx)\n",
    "        A = trtrs(L, k_xs)\n",
    "        V = trtrs(L, self.y)\n",
    "        mean_f = torch.mm(torch.transpose(A, 0, 1), V)        \n",
    "        if self.meanfunction is not None:\n",
    "            mean_f += self.meanfunction(inputs_new,self.y.shape[1])            \n",
    "        A = trtrs(L, k_xs)\n",
    "        if diag:\n",
    "            var_f1 = self.kernel.K_diag(inputs_new)\n",
    "            var_f2 = torch.sum(A * A, 0)\n",
    "            return mean_f, (var_f1 - var_f2).reshape(-1,1)\n",
    "        else:\n",
    "            var_f1 = self.kernel.K(inputs_new)\n",
    "            var_f2 = torch.mm(A.t(), A)\n",
    "            return mean_f, (var_f1 - var_f2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/Validation/Synthesis8_10/'\n",
    "filename = 'SinCos_state_8_1'\n",
    "formatted = '.mat'\n",
    "Synthetic = sio.loadmat(data_path + filename + formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,x,y):\n",
    "        self.data_len = x_train.shape[1]\n",
    "        self.to_tensor = torch.DoubleTensor\n",
    "        self._transform_list(x,y)\n",
    "        \n",
    "    def _transform_list(self,x,y):        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for i_th in range(x.shape[1]):\n",
    "            x_list.append(x[:,i_th])\n",
    "            y_list.append(y[:,i_th])\n",
    "            \n",
    "        self.x = x_list\n",
    "        self.y = y_list\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        x_ith = self.x[index]\n",
    "        y_ith = self.y[index]\n",
    "        return (self.to_tensor(x_ith),self.to_tensor(y_ith))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Synthetic['zfull']\n",
    "x = Synthetic['xfull']\n",
    "y = Synthetic['yfull']\n",
    "\n",
    "num_train = 50\n",
    "x_train,x_test = x[:,:num_train],x[:,num_train:2*num_train]\n",
    "y_train,y_test = y[:,:num_train],y[:,num_train:2*num_train]\n",
    "\n",
    "train_set = custom_dataset(x_train,y_train)\n",
    "test_set = custom_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_set, batch_size = 1, shuffle = False, num_workers = 2)\n",
    "#testloader = DataLoader(test_set, batch_size = 1, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs, psd = signal.welch(y_train.reshape(1, -1).squeeze(), fs=int(1 / (x_train[1] - x_train[0])), nperseg=y_train.shape[0])\n",
    "# psd = psd / np.sum(psd)\n",
    "\n",
    "# #Q = 10\n",
    "# A = np.argsort(-psd)[:Q]\n",
    "# #num_sample = 10\n",
    "# #num_sample2 = Q * num_sample\n",
    "# #data_dim = 1\n",
    "\n",
    "# param_dict['sf2'] = 1.0\n",
    "# param_dict['pi'] = psd[A].reshape(-1, 1)\n",
    "# # param_dict['mu'] =  .5*np.random.rand(A.shape[0]).reshape(-1, 1).reshape(-1, data_dim)\n",
    "\n",
    "# if data_dim == 1:\n",
    "#     param_dict['mu'] = freqs[A].reshape(-1, 1).reshape(-1, data_dim)\n",
    "#     param_dict['mu_prior'] = freqs[A].reshape(-1, 1).reshape(-1, data_dim)\n",
    "# else:\n",
    "#     param_dict['mu'] = np.random.rand(Q * data_dim, 1).reshape(-1, data_dim)\n",
    "#     param_dict['mu_prior'] = np.random.rand(Q * data_dim, 1).reshape(-1, data_dim)\n",
    "\n",
    "# param_dict['std'] = .01 * np.random.rand(Q * data_dim, 1).reshape(-1, data_dim)\n",
    "# param_dict['std_prior'] = .01 * np.random.rand(Q * data_dim, 1).reshape(-1, data_dim)\n",
    "\n",
    "# param_dict['theta'] = .1 * np.random.randn(Q * data_dim, 1).reshape(-1, data_dim)\n",
    "# param_dict['skew'] = .1 * np.random.uniform(-1.0, 1.0, Q).reshape(-1, 1)\n",
    "# param_dict['stable'] = np.random.uniform(0.01, 1.99, Q).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import signal\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# top_Q = 5\n",
    "# init_std = 0.1\n",
    "# maximal_hidden_state = 5\n",
    "\n",
    "# psd_list = []\n",
    "# freq_list = []\n",
    "# for i, (x_i, y_i) in enumerate(trainloader):\n",
    "#     freqs,psd = signal.welch(y_i.reshape(1, -1).squeeze(), fs=int(1 / (x_i[0,1] - x_i[0,0])), nperseg=y_i.squeeze().shape[0])\n",
    "#     psd_list.append(psd/psd.sum())\n",
    "# psd_list = np.asarray(psd_list)\n",
    "\n",
    "# param_list_dict = []\n",
    "# Kmeans_instance = KMeans(n_clusters = maximal_hidden_state, random_state = 0).fit(psd_list)\n",
    "# for j_th,j_th_centers in enumerate(Kmeans_instance.cluster_centers_):\n",
    "#     #print(j_th_centers)\n",
    "#     A = np.argsort(-j_th_centers)[:top_Q]\n",
    "#     param_list_dict.append({'num_Q' : top_Q,\n",
    "#                             'init_pi' : j_th_centers[A] , \n",
    "#                             'init_mu' : freqs[A] ,\n",
    "#                             'init_std' :init_std*np.random.rand(freqs[A].shape[0]) \n",
    "#                            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def _init_param_emission_model(trainloader , maximal_hidden_state,top_Q,init_std) : \n",
    "    psd_list = []\n",
    "    freq_list = []\n",
    "    for i, (x_i, y_i) in enumerate(trainloader):\n",
    "        freqs,psd = signal.welch(y_i.reshape(1, -1).squeeze(), fs=int(1 / (x_i[0,1] - x_i[0,0])), nperseg=y_i.squeeze().shape[0])\n",
    "        psd_list.append(psd/psd.sum())\n",
    "    psd_list = np.asarray(psd_list)\n",
    "\n",
    "    param_list_dict = []\n",
    "    Kmeans_instance = KMeans(n_clusters = maximal_hidden_state, random_state = 0).fit(psd_list)\n",
    "    for j_th,j_th_centers in enumerate(Kmeans_instance.cluster_centers_):\n",
    "        #print(j_th_centers)\n",
    "        A = np.argsort(-j_th_centers)[:top_Q]\n",
    "        param_list_dict.append({'num_Q' : top_Q,\n",
    "                                'init_pi' : j_th_centers[A] , \n",
    "                                'init_mu' : freqs[A] ,\n",
    "                                'init_std' :init_std*np.random.rand(freqs[A].shape[0]) \n",
    "                               })\n",
    "    return param_list_dict , Kmeans_instance\n",
    "\n",
    "# fig = plt.figure(figsize = (20,10))\n",
    "# for j_th_centers in Kmeans_instance.cluster_centers_:\n",
    "#     #plt.plot(freqs,j_th_centers)\n",
    "#     plt.plot(freqs,np.log(j_th_centers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_Q = 5\n",
    "init_std = 0.1\n",
    "maximal_hidden_state = 5\n",
    "param_list_dict,_ = _init_param_emission_model(trainloader , maximal_hidden_state,top_Q,init_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernels.SM_kernel import SM\n",
    "\n",
    "\n",
    "def _make_gp_emission(model_name,x_train,y_train,param_dict):\n",
    "    input_dim = 1\n",
    "    cuda_option = True\n",
    "    Kern = SM(param_dict['num_Q'], input_dim,\n",
    "              param_dict['init_pi'],\n",
    "              param_dict['init_mu'],\n",
    "              param_dict['init_std'],\n",
    "              ARD=True,\n",
    "              cuda_option= cuda_option)\n",
    "\n",
    "    if model_name == 'gpr_sm':\n",
    "        model_gpr = gpr(x_train, y_train,\n",
    "                        Kern,\n",
    "                        mean_function=None,\n",
    "                        likelihood=None,\n",
    "                        cuda_option= cuda_option)\n",
    "\n",
    "        return model_gpr\n",
    "\n",
    "    if model_name == 'ssgpr_reg':\n",
    "        model_ssgpr_reg = ssgpr_amoritized_reparametrization_regulaizer(x_train, y_train,\n",
    "                                                                        num_sample,\n",
    "                                                                        param_dict,\n",
    "                                                                        cuda_option= cuda_option)\n",
    "\n",
    "        return model_ssgpr_reg\n",
    "\n",
    "\n",
    "def _construct_emission_model(model_name,param_list_dict):\n",
    "    emission_model_list = []\n",
    "    for i_th_param_dict in param_list_dict:\n",
    "        emission_model_list.append(_make_gp_emission(model_name, None, None,i_th_param_dict))\n",
    "    return emission_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpr_sm'\n",
    "emission_model_list = _construct_emission_model(model_name,param_list_dict)\n",
    "#emission_model_list[0],emission_model_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GP_Emission(torch.nn):\n",
    "    \n",
    "#     def __init__(self,)\n",
    "#     def _update_hyp_param(self):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.from_numpy(np.asarray([.1,.2,.3])).view(-1,1)\n",
    "A,torch.logsumexp(A.log(),dim = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.from_numpy(np.asarray([[.1,.2,.3],[.4,.5,.6]])).view(-1,1)\n",
    "A,torch.logsumexp(A.log(),dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_var_param_A = repmat(model_class.prior_A ,model_class.state_num,1) ;\n",
    "# temp_var_param_pi = model_class.prior_pi;\n",
    "\n",
    "# Astar = exp( digamma(temp_var_param_A) - digamma( repmat(sum(temp_var_param_A,2),[1,model_class.state_num]) ) );\n",
    "# Pistar = exp( digamma(temp_var_param_pi) - digamma(sum(temp_var_param_pi)) );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emission_GP(Module):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module,Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class HMM_EmissionGP(Module):\n",
    "    \n",
    "    def __init__(self,emission_model_list,num_hidden_state):\n",
    "        super(HMM_EmissionGP,self).__init__()        \n",
    "        self.num_hidden_state = num_hidden_state        \n",
    "        self.emission_model_list = emission_model_list\n",
    "        self._init_param()\n",
    "    \n",
    "    def _init_param(self):\n",
    "        # intilaize variational param\n",
    "        self.prior_A = torch.from_numpy(np.ones([num_hidden_state,1]))\n",
    "        self.prior_pi = torch.from_numpy(np.ones([num_hidden_state,1])) \n",
    "        self.var_param_A = self.prior_A.repeat(self.num_hidden_state,1)\n",
    "        self.var_param_pi = self.prior_pi\n",
    "            \n",
    "        # initialize hyperparam gp\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def _run_Estep(self, batch_x, batch_y):\n",
    "        batch_log_obs_prob = self._calc_obs_prob(batch_x,batch_y)\n",
    "        A_star = torch.diagmma( self.var_param_A ) - torch.diagmma( self.var_param_A.sum(dim =1).repeat(1,self.num_hidden_state) )\n",
    "        A_star.exp_()\n",
    "        pi_star = torch.digamma( self.var_param_pi ) - torch.digamma(self.var_param_pi.sum());\n",
    "        pi_star.exp_()       \n",
    "        log_gamma,log_qsi,log_C,log_lik = self._log_forward_backward( batch_log_obs_prob, pi_star , A_star)\n",
    "        gamma,qsi = self._log_to_origin_scale(log_gamma,log_qsi)        \n",
    "        return gamma, qsi\n",
    "\n",
    "\n",
    "    \n",
    "    def _run_Mstep_EM(self):\n",
    "        self._update_var_param_A(gamma,qsi)        \n",
    "        self._update_var_param_pi(gamma,qsi)        \n",
    "        self._update_hyp_param_emission_model()\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def _calc_obs_prob(self,batch_x,batch_y):        \n",
    "        batch_log_emission_prob = torch.from_numpy(np.zeros(self.num_hidden_state,batch_y.shape[1]))\n",
    "        \n",
    "        for i_th,(i_th_batch_x,i_th_batch_y) in enumerate(zip(batch_x,batch_y)) :\n",
    "                for j_th,j_th_emission_model in enumerate(self.emission_model_list):\n",
    "                    batch_emission_prob[j_th,i_th] = j_th_emission_model._get_log_prob(i_th_batch_x,i_th_batch_y)\n",
    "            \n",
    "        return batch_log_emission_prob\n",
    "    \n",
    "    \n",
    "    def _log_forward(self, log_batch_obs_prob, pi_star , A_star ):\n",
    "        _ , batch_seq_length = log_batch_obs_prob.shape\n",
    "        \n",
    "        log_A_star = A_star.log()\n",
    "        log_pi_star = pi_star.log()        \n",
    "        log_alpha = torch.from_numpy(np.zeros([self.num_hidden_state,batch_seq_length]))        \n",
    "        log_C = torch.from_numpy( np.zeros( [ batch_seq_length ] ) )\n",
    "        log_C[0] = torch.logsumexp( log_alpha[:,0:0+1] + log_pi_star , dim = 0)\n",
    "        log_alpha[:,0:0+1] = -log_C[0] + log_batch_obs_prob[:,0:0+1] + log_pi_star\n",
    "        \n",
    "        for i_th in range(1,batch_seq_length):    \n",
    "            temp_alpha = log_A_star + log_alpha[:,i_th-1:i_th] + log_batch_obs_prob[:,i_th:i_th+1]\n",
    "            log_C[i_th] = torch.logsumexp(temp_alpha.view(-1,1),dim = 0)            \n",
    "            for j_th in range(self.num_hidden_state):\n",
    "                temp_ji = log_alpha[:,i_th-1:i_th] + log_A_star[:,j_th:j_th+1]\n",
    "                log_alpha[j_th,i_th] = -log_C[i_th] + log_batch_obs_prob[j_th,i_th] + torch.logsumexp(temp_ji.view(-1,1),dim = 0)\n",
    "        \n",
    "        return log_alpha,log_C\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _log_backward(self, log_batch_obs_prob, A_star, log_C ):\n",
    "        _ , batch_seq_length = log_batch_obs_prob.shape\n",
    "        \n",
    "        log_At_star = A_star.log().t()        \n",
    "        log_beta = torch.from_numpy(np.zeros([self.num_hidden_state,batch_seq_length]))        \n",
    "        for i_th in reversed(range(batch_seq_length - 1)):\n",
    "            for j_th in range(self.num_hidden_state):\n",
    "                temp_beta = log_beta[:,i_th+1:i_th+2] + log_batch_obs_prob[:,i_th+1:i_th+2] + log_At_star[:,j_th:j_th+1]\n",
    "                log_beta[j_th,i_th] = -log_C[i_th + 1] + torch.logsumexp(temp_beta.view(-1,1) , dim = 0)\n",
    "        return log_beta\n",
    "    \n",
    "    \n",
    "    def _log_forward_backward(self, log_batch_obs_prob, pi_star , A_star):\n",
    "        _ , batch_seq_length = log_batch_obs_prob.shape        \n",
    "        log_A_star = A_star.log()\n",
    "        log_pi_star = pi_star.log()        \n",
    "        log_alpha,log_C = self._log_forward( log_batch_obs_prob, pi_star , A_star)\n",
    "        log_beta = self._log_backward(log_batch_obs_prob, A_star, log_C )\n",
    "        log_gamma = log_alpha + log_beta\n",
    "        \n",
    "        log_qsi = torch.from_numpy(np.zeros([self.num_hidden_state,self.num_hidden_state,batch_seq_length]))\n",
    "        for i_th in range(1, batch_seq_length ):\n",
    "            log_qsi[:,:,i_th] = -log_C[i_th] + log_A_star + log_alpha[:,i_th:i_th+1]\n",
    "            log_qsi[:,:,i_th] += (log_beta[:,i_th:i_th+1] + log_batch_obs_prob[:,i_th:i_th+1]).t()\n",
    "    \n",
    "        log_lik = log_C.sum()\n",
    "        return log_gamma,log_qsi,log_C,log_lik \n",
    "    \n",
    "\n",
    "    def _log_to_origin_scale(self,log_gamma,log_qsi):        \n",
    "        _,batch_seq_length = log_gamma.shape\n",
    "        for i_th in range(batch_seq_length):\n",
    "            log_gamma[:,i_th] = log_gamma[:,i_th] - log_gamma[:,i_th].max()    \n",
    "        gamma = log_gamma.exp()\n",
    "        gamma = gamma.div(gamma.sum(dim = 0))\n",
    "        qsi = log_qsi.exp()\n",
    "        return gamma,qsi\n",
    "\n",
    "    def _update_var_param_A(self):\n",
    "        return \n",
    "    \n",
    "    def _update_var_param_pi(self):\n",
    "        return \n",
    "    \n",
    "    def _update_hyp_param_emission_model(self):\n",
    "        return\n",
    "    \n",
    "#     def _update_var_nat_param_EM(self):\n",
    "#         return    \n",
    "#     def _update_var_nat_param_VBEM(self):\n",
    "#         return\n",
    "#     def _update_var_nat_param_SVI(self):\n",
    "#         return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_vbem(model_cls):\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_state = 10\n",
    "batch_seq_length = 100\n",
    "log_obs_prob = torch.from_numpy(np.random.rand(num_hidden_state,batch_seq_length)).log()\n",
    "pi_star = torch.from_numpy(np.random.rand(num_hidden_state,1))\n",
    "A_star =  torch.from_numpy(np.random.rand(num_hidden_state,num_hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM  = HMM_EmissionGP(emission_model_list = emission_model_list,num_hidden_state = num_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5f3babee1e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHMM_EmissionGP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_obs_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_x' is not defined"
     ]
    }
   ],
   "source": [
    "HMM_EmissionGP._calc_obs_prob(batch_x,batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_alpha,log_C = HMM._log_forward(log_obs_prob, pi_star , A_star )\n",
    "# log_beta = HMM._log_backward( log_obs_prob, A_star , log_C )\n",
    "log_gamma,log_qsi,log_C,log_lik  = HMM._log_forward_backward( log_obs_prob, pi_star , A_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma,qsi = HMM._log_to_origin_scale(log_gamma,log_qsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
